{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrXMyliFG26n",
        "outputId": "c3042ef0-52d0-4e77-ad08-0a3727381b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_bN1kuegef6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQDN45yxhYiZ"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self,\n",
        "                 datapath,\n",
        "                 shuffle=True):\n",
        "        self.shuffle = shuffle\n",
        "        self.data = {}\n",
        "\n",
        "        for root, _, files in os.walk(datapath):\n",
        "            for name in files:\n",
        "                filename = name.split('.')\n",
        "                self.data[filename[0] + filename[1]] = np.load(os.path.join(root, name))\n",
        "\n",
        "    def __call__(self):\n",
        "        keys = list(self.data.keys())\n",
        "        if self.shuffle:\n",
        "            random.shuffle(keys)\n",
        "        # итерация по ключам в словаре self.data\n",
        "        for key in keys:\n",
        "            # если маска целевого значения пустая, то пропускаем пример\n",
        "            if np.all(self.data[key][:,:,-1,:,:] == 0.):\n",
        "                continue\n",
        "            seq = self.__getSequence(key)\n",
        "            arrays = []\n",
        "            badCount = 0\n",
        "            # итерация по историческим данным\n",
        "            for item in seq:\n",
        "                # некоторые исторические данные могут отсутствовать\n",
        "                try:\n",
        "                    if np.all(self.data[item][:,:,-1,:,:] == 0.):\n",
        "                        badCount += 1\n",
        "                    arrays.append(self.data[item])\n",
        "                except KeyError:\n",
        "                    # print(f'No key: {item}')\n",
        "                    badCount += 1\n",
        "                    arrays.append(np.zeros_like(self.data[key]))\n",
        "            # если пропусков в данных больше чем 5%, то пропускаем пример\n",
        "            # print(f'Bad count: {badCount}')\n",
        "            if badCount / len(arrays) > 0.3:\n",
        "                continue\n",
        "            else:\n",
        "                x = np.concatenate(arrays, axis=1)\n",
        "                y = self.data[key]\n",
        "                for beam in range(16):\n",
        "                    yield x[:,:,:,beam,0], y[:,:,:-1,beam,0]\n",
        "    \n",
        "    def __getSequence(self, key):\n",
        "        keyDT = datetime.strptime(key, '%Y%m%d%H%M')\n",
        "        # список массивов периодов за месяц\n",
        "        monthBefore = []\n",
        "        for i in range(30, 1, -1):\n",
        "            daysBefore = (keyDT-timedelta(days=i)).strftime('%Y%m%d%H%M')\n",
        "            monthBefore.append(daysBefore)\n",
        "        # список массивов периодов за день до целевого массива\n",
        "        dayBefore = []\n",
        "        for i in range(24, 0, -2):\n",
        "            hoursBefore = (keyDT-timedelta(hours=i)).strftime('%Y%m%d%H%M')\n",
        "            dayBefore.append(hoursBefore)\n",
        "\n",
        "        return monthBefore + dayBefore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BzyJ5oyxT9Y"
      },
      "outputs": [],
      "source": [
        "datapath = 'drive/MyDrive/2002-train'\n",
        "\n",
        "loader = DataLoader(datapath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_R6N6Gc4vti",
        "outputId": "e95bb595-fb6e-4d57-fcbe-0d55f8a9311c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No key: 200205241400\n",
            "No key: 200205251400\n",
            "No key: 200205261400\n",
            "No key: 200206071400\n",
            "No key: 200206081400\n",
            "No key: 200206091400\n",
            "No key: 200206101400\n",
            "No key: 200206111400\n",
            "No key: 200206121400\n",
            "No key: 200206191400\n",
            "No key: 200206221600\n",
            "No key: 200206221800\n",
            "No key: 200206222000\n",
            "No key: 200206222200\n",
            "Bad count: 14\n",
            "No key: 200206260800\n",
            "No key: 200207051200\n",
            "No key: 200207051400\n",
            "No key: 200207051600\n",
            "No key: 200207051800\n",
            "No key: 200207052000\n",
            "No key: 200207052200\n",
            "Bad count: 7\n",
            "(70, 2460, 7) (70, 60, 6)\n"
          ]
        }
      ],
      "source": [
        "for x, y in loader:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nAEVfCH39wI",
        "outputId": "b67b1d38-1fde-4400-be14-7b609327ba5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+++++++++++++++++++++++++++++++++++++++ Epoch 0 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 2.403637409210205 d_loss: 0.04296540468931198 g_mae: 42970.83984375 val_mae: 15377.775390625\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 1 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 1.539368987083435 d_loss: 0.022736212238669395 g_mae: 43092.8359375 val_mae: 15393.7548828125\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 2 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 1.2651041746139526 d_loss: 0.02319999225437641 g_mae: 43002.11328125 val_mae: 15363.9052734375\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 3 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.9658921957015991 d_loss: 0.017713144421577454 g_mae: 43107.9921875 val_mae: 15382.193359375\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 4 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.7727137804031372 d_loss: 0.014178609475493431 g_mae: 43039.65234375 val_mae: 15362.5576171875\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 5 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.6439297795295715 d_loss: 0.011850724928081036 g_mae: 43115.87890625 val_mae: 15383.9912109375\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 6 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.5519397854804993 d_loss: 0.010159566067159176 g_mae: 43080.4296875 val_mae: 15376.345703125\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 7 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.4829598665237427 d_loss: 0.008890336379408836 g_mae: 43116.19140625 val_mae: 15383.787109375\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 8 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.42929765582084656 d_loss: 0.00790261011570692 g_mae: 43083.01953125 val_mae: 15375.265625\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 9 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.3868710398674011 d_loss: 0.007122655399143696 g_mae: 43112.52734375 val_mae: 15382.1015625\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 10 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.3517056107521057 d_loss: 0.006477164104580879 g_mae: 43085.09765625 val_mae: 15375.0908203125\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 11 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.32240352034568787 d_loss: 0.00593853322789073 g_mae: 43133.22265625 val_mae: 15388.9453125\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 12 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.2976032495498657 d_loss: 0.005488562397658825 g_mae: 43113.4609375 val_mae: 15384.603515625\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 13 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.2763482630252838 d_loss: 0.0052108378149569035 g_mae: 43119.27734375 val_mae: 15386.2646484375\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 14 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.2579250633716583 d_loss: 0.004863461945205927 g_mae: 43102.6953125 val_mae: 15382.3134765625\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 15 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.24219633638858795 d_loss: 0.007839455269277096 g_mae: 43114.74609375 val_mae: 15384.982421875\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 16 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.22879071533679962 d_loss: 0.0073783197440207005 g_mae: 43115.15625 val_mae: 15385.3349609375\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 17 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.2160836011171341 d_loss: 0.00697282375767827 g_mae: 43145.89453125 val_mae: 15395.4423828125\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 18 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.20471078157424927 d_loss: 0.006606148090213537 g_mae: 43126.81640625 val_mae: 15389.9521484375\n",
            "+++++++++++++++++++++++++++++++++++++++ Epoch 19 +++++++++++++++++++++++++++++++++++++++\n",
            " D step: 4181 \n",
            " G step: 4181 \n",
            " MAE step: 4181 \n",
            " VAL_MAE step: 1461 \n",
            "g_loss: 0.19455429911613464 d_loss: 0.006276359315961599 g_mae: 43140.8671875 val_mae: 15393.1201171875\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n",
        "\n",
        "# генератор\n",
        "def get_generator():\n",
        "    inp = layers.Input(shape=(70, 2460, 7))\n",
        "    conv = layers.Conv2D(128, kernel_size=(1, 2401), activation='tanh')(inp)\n",
        "    norm = layers.BatchNormalization()(conv)\n",
        "    out = layers.Dense(6, activation='tanh')(norm)\n",
        "    model = Model(inp, out)\n",
        "    return model\n",
        "\n",
        "# дискриминатор\n",
        "def get_discriminator():\n",
        "    hist_inp = layers.Input(shape=(70, 2460, 7))\n",
        "    hist_conv = layers.Conv2D(filters=6, kernel_size=(1, 1))(hist_inp)\n",
        "    gen_out = layers.Input(shape=(70, 60, 6))\n",
        "    joined = layers.Concatenate(axis=2)([hist_conv, gen_out])\n",
        "    conv = layers.Conv2D(filters=128, kernel_size=(1, 2461), activation='tanh')(joined)\n",
        "    conv = layers.Conv2D(filters=6, kernel_size=(1, 1), activation='tanh', padding='same')(conv)\n",
        "    norm = layers.BatchNormalization()(conv)\n",
        "    flat = layers.Flatten()(norm)\n",
        "    dense = layers.Dense(4)(flat)\n",
        "    reshape = layers.Reshape((2, 2, 1))(dense)\n",
        "    out = layers.Dense(1, activation='sigmoid')(reshape)\n",
        "    model = Model([hist_inp, gen_out], out)\n",
        "    return model\n",
        "\n",
        "# данные\n",
        "data_path = 'drive/MyDrive/2002-train'\n",
        "val_data_path = 'drive/MyDrive/2002-val'\n",
        "\n",
        "train_generator = DataLoader(data_path)\n",
        "val_generator = DataLoader(val_data_path)\n",
        "\n",
        "batch_size = 8\n",
        "epochs = 20\n",
        "\n",
        "dataset = tf.data.Dataset.from_generator(train_generator,\n",
        "                                         output_types=(tf.float64, tf.float64)).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_generator(val_generator,\n",
        "                                         output_types=(tf.float64, tf.float64)).batch(batch_size)\n",
        "\n",
        "\n",
        "# непосредственно GAN\n",
        "gen = get_generator()\n",
        "dis = get_discriminator()\n",
        "\n",
        "gen.compile()\n",
        "dis.compile()\n",
        "\n",
        "d_optimizer=tf.keras.optimizers.Adam(lr=0.0001)\n",
        "g_optimizer=tf.keras.optimizers.Adam(lr=0.0001)\n",
        "\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "mae = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "gen_loss_tracker = tf.keras.metrics.Mean(name='generator_loss')\n",
        "disc_loss_tracker = tf.keras.metrics.Mean(name='discriminator_loss')\n",
        "gen_mae_tracker = tf.keras.metrics.Mean(name='generator_mae')\n",
        "val_mae_tracker = tf.keras.metrics.Mean(name='generator_validation_mae')\n",
        "\n",
        "history = []\n",
        "old_gloss=1e100\n",
        "old_dloss=1e100\n",
        "old_mae=1e100\n",
        "old_val=1e100\n",
        "min_val=1e100\n",
        "\n",
        "block_mask_shape = (2, 2)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    g_mae = 0\n",
        "    val_mae = 0\n",
        "    \n",
        "    print(f'+++++++++++++++++++++++++++++++++++++++ Epoch {epoch} +++++++++++++++++++++++++++++++++++++++')\n",
        "\n",
        "    # обучение дискриминатора\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n",
        "        print('\\r D step:', step, end=' ')\n",
        "\n",
        "        y_shape = tf.shape(y_batch_train)\n",
        "        if y_shape[0] != batch_size:\n",
        "            break\n",
        "        \n",
        "        cmp_mask = tf.math.round(tf.random.uniform((y_shape[0], block_mask_shape[0], block_mask_shape[1]), minval=0, maxval=1, dtype=tf.dtypes.float64))\n",
        "        true_mask = np.zeros(y_shape[:-1])\n",
        "\n",
        "        height = true_mask[0].shape[0] // block_mask_shape[0]\n",
        "        width = true_mask[0].shape[1] // block_mask_shape[1]\n",
        "\n",
        "        for k in range(y_shape[0]):\n",
        "            for i in range(block_mask_shape[0]):\n",
        "                for j in range(block_mask_shape[1]):\n",
        "                    val = cmp_mask[k, i, j]\n",
        "                    true_mask[k, i*height:(i+1)*height, j*width:(j+1)*width].fill(val)\n",
        "        \n",
        "        fake_mask = tf.math.subtract(tf.ones(shape=y_shape[:-1], dtype=tf.dtypes.float64), true_mask)        \n",
        "        true_mask_6 = tf.repeat(tf.expand_dims(true_mask, axis=-1), 6, axis=-1, name=None)\n",
        "        fake_mask_6 = tf.repeat(tf.expand_dims(fake_mask, axis=-1), 6, axis=-1, name=None)\n",
        "\n",
        "        x, y = x_batch_train, y_batch_train\n",
        "\n",
        "        generated = tf.cast(gen(x), dtype=tf.dtypes.float64)\n",
        "        # print(f'y shape: {y.shape}, true mask 6 shape: {true_mask_6.sha2 pe}')\n",
        "        d_input_mixed = tf.math.add(tf.math.multiply(true_mask_6, y), \n",
        "                                tf.math.multiply(fake_mask_6, generated))\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction_mask = dis([x, d_input_mixed])\n",
        "            d_loss = loss_fn(cmp_mask, prediction_mask)\n",
        "            grads = tape.gradient(d_loss, dis.trainable_weights)\n",
        "            d_optimizer.apply_gradients(zip(grads, dis.trainable_weights))\n",
        "\n",
        "    print()\n",
        "\n",
        "    # обучение генератора\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n",
        "        print('\\r G step:', step, end=' ')\n",
        "\n",
        "        y_shape = tf.shape(y_batch_train)\n",
        "        if y_shape[0] != batch_size:\n",
        "            break\n",
        "\n",
        "        x, y = x_batch_train, y_batch_train\n",
        "        misleading_mask = tf.ones(shape=(y_shape[0], block_mask_shape[0], block_mask_shape[1]), dtype=tf.dtypes.float64)\n",
        "\n",
        "        # обучение генератора, без обновления весов дискриминатора\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_forcast = gen(x)\n",
        "            prediction_mask = dis([x, fake_forcast])\n",
        "            g_loss = loss_fn(misleading_mask, prediction_mask)\n",
        "            loss_value = g_loss\n",
        "            # g_mae = mae(y, fake_forcast)\n",
        "            # loss_value = 0.9*g_loss+0.1*g_mae\n",
        "            # loss_value += sum(generator.losses)\n",
        "            grads = tape.gradient(loss_value, gen.trainable_weights)\n",
        "            g_optimizer.apply_gradients(zip(grads, gen.trainable_weights))\n",
        "    \n",
        "    print()\n",
        "\n",
        "    # MAE\n",
        "    g_mae = 0\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(dataset):\n",
        "        print('\\r MAE step:', step, end=' ')\n",
        "\n",
        "        y_shape = tf.shape(y_batch_train)\n",
        "        if y_shape[0] != batch_size:\n",
        "            break\n",
        "\n",
        "        x, y = x_batch_train, y_batch_train\n",
        "        fake_forcast = gen(x)\n",
        "        g_mae += mae(y, fake_forcast)\n",
        "    \n",
        "    print()\n",
        "\n",
        "    # VAL MAE\n",
        "    val_mae = 0\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(val_dataset):\n",
        "        print('\\r VAL_MAE step:', step, end=' ')\n",
        "\n",
        "        y_shape = tf.shape(y_batch_train)\n",
        "        if y_shape[0] != batch_size:\n",
        "            break\n",
        "        \n",
        "        x, y = x_batch_train, y_batch_train\n",
        "        fake_forcast = gen(x)\n",
        "        val_mae += mae(y, fake_forcast)\n",
        "    \n",
        "    print()\n",
        "\n",
        "    # monitor loss\n",
        "    gen_loss_tracker.update_state(g_loss)\n",
        "    disc_loss_tracker.update_state(d_loss)\n",
        "    gen_mae_tracker.update_state(g_mae)\n",
        "    val_mae_tracker.update_state(val_mae)\n",
        "\n",
        "    print(\"g_loss:\",float(gen_loss_tracker.result()),\n",
        "          \"d_loss:\", float(disc_loss_tracker.result()),\n",
        "          \"g_mae:\", float(gen_mae_tracker.result()),\n",
        "          \"val_mae:\", float(val_mae_tracker.result()))\n",
        "    \n",
        "    history.append([float(gen_loss_tracker.result()),\n",
        "                    float(disc_loss_tracker.result()),\n",
        "                    float(gen_mae_tracker.result()),\n",
        "                    float(val_mae_tracker.result())])\n",
        "    \n",
        "    if float(val_mae_tracker.result()) < min_val:\n",
        "        min_val = val_mae_tracker.result()\n",
        "        gen.save('drive/MyDrive/best_generator.hdf5')\n",
        "        dis.save('drive/MyDrive/best_discriminator.hdf5')\n",
        "        best_generator = gen\n",
        "        best_discriminator = dis\n",
        "\n",
        "    old_dloss = float(disc_loss_tracker.result())\n",
        "    old_gloss = float(gen_loss_tracker.result())\n",
        "    old_mae = float(gen_mae_tracker.result())\n",
        "    old_val = float(val_mae_tracker.result())\n",
        "\n",
        "with open('drive/MyDrive/log.txt', 'w') as fp:\n",
        "    for record in history:\n",
        "        fp.write(f'{record[0]} {record[1]} {record[2]} {record[3]}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}