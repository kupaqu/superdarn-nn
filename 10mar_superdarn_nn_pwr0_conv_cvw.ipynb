{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kupaqu/superdarn-nn/blob/main/10mar_superdarn_nn_pwr0_conv_cvw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSp8ZGqI3vCO",
        "outputId": "b819e6f4-4c94-447e-c0ce-76fb8c839020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5WmZRpa308m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeaZW0zh33ny"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "    def __init__(self,\n",
        "                 datapath,\n",
        "                 shuffle=True):\n",
        "        self.shuffle = shuffle\n",
        "        self.data = {}\n",
        "\n",
        "        for root, _, files in os.walk(datapath):\n",
        "            for name in files:\n",
        "                filename = name.split('.')\n",
        "                arr = np.load(os.path.join(root, name))\n",
        "                self.data[filename[0] + filename[1][:2]] = arr\n",
        "\n",
        "    def __iter__(self):\n",
        "        target_datetime = list(self.data.keys())\n",
        "        if self.shuffle:\n",
        "            random.shuffle(target_datetime)\n",
        "\n",
        "        # итерация по ключам в словаре self.data, где ключи – название файла\n",
        "        for key in target_datetime:\n",
        "            \n",
        "            seq = self.__getSequence(key)\n",
        "            arrays = []\n",
        "            badCount = 0\n",
        "\n",
        "            # итерация по историческим данным\n",
        "            for item in seq:\n",
        "                # некоторые исторические данные могут отсутствовать\n",
        "                try:\n",
        "                    arrays.append(self.data[item])\n",
        "                except KeyError:\n",
        "                    badCount += 1\n",
        "            \n",
        "            # если есть пропуски, то пропускаем пример\n",
        "            if badCount != 0:\n",
        "                continue\n",
        "            else:\n",
        "                x = np.concatenate(arrays, axis=1)\n",
        "                y = self.data[key]\n",
        "                yield x, y\n",
        "\n",
        "    def __getSequence(self, key):\n",
        "        filename_datetime = datetime.strptime(key, '%Y%m%d%H')\n",
        "\n",
        "        # список массивов за день до целевого массива\n",
        "        dayBefore = []\n",
        "        for i in range(24, 0, -2):\n",
        "            hoursBefore = (filename_datetime-timedelta(hours=i)).strftime('%Y%m%d%H')\n",
        "            dayBefore.append(hoursBefore)\n",
        "\n",
        "        # тот же час, но за неделю до целевого массива\n",
        "        weekBeforeInThatHour = []\n",
        "        for i in range(7, 0, -1):\n",
        "            thatHour = (filename_datetime-timedelta(days=i)).strftime('%Y%m%d%H')\n",
        "            weekBeforeInThatHour.append(thatHour)\n",
        "\n",
        "        return dayBefore + weekBeforeInThatHour\n",
        "\n",
        "        # список массивов за неделю до целевого массива\n",
        "        # weekBefore = []\n",
        "        # for i in range(24*7, 0, -2):\n",
        "        #     hoursBefore = (filename_datetime-timedelta(hours=i)).strftime('%Y%m%d%H')\n",
        "        #     weekBefore.append(hoursBefore)\n",
        "        \n",
        "        # return weekBefore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STl8wSev4VR0"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "\n",
        "train_loader = DataLoader('drive/MyDrive/2018-train')\n",
        "val_loader = DataLoader('drive/MyDrive/2018-val')\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(train_loader, output_types=(tf.float64, tf.float64)).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_generator(val_loader, output_types=(tf.float64, tf.float64)).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_sz1Y9Y4B8E"
      },
      "outputs": [],
      "source": [
        "# генератор\n",
        "def get_generator(regularizer_lambda=1e-5):\n",
        "\n",
        "    history = tf.keras.layers.Input(shape=(100, 5040, 1))\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=5,\n",
        "        activation='relu',\n",
        "        padding='same',\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(regularizer_lambda)\n",
        "    )(history)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(1, 7),\n",
        "        dilation_rate=(1, 12*60),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(regularizer_lambda)\n",
        "    )(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=5,\n",
        "        activation='relu',\n",
        "        padding='same',\n",
        "    )(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(1, 12),\n",
        "        dilation_rate=(1, 60),\n",
        "        activation='relu',\n",
        "    )(x)\n",
        "\n",
        "    output = tf.keras.layers.Conv2D(\n",
        "        filters=1,\n",
        "        kernel_size=1,\n",
        "        activation='linear'\n",
        "    )(x)\n",
        "\n",
        "    generator = tf.keras.models.Model(history, output, name='generator')\n",
        "\n",
        "    return generator\n",
        "\n",
        "# дискриминатор\n",
        "def get_discriminator(regularizer_lambda=1e-5):\n",
        "\n",
        "    history = tf.keras.layers.Input(shape=(100, 5040, 1))\n",
        "    target = tf.keras.layers.Input(shape=(100, 60, 1))\n",
        "    concat = tf.keras.layers.Concatenate(axis=2)([history, target])\n",
        "    \n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=5,\n",
        "        activation='relu',\n",
        "        padding='same',\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(regularizer_lambda)\n",
        "    )(concat)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(1, 7),\n",
        "        dilation_rate=(1, 12*60),\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(regularizer_lambda)\n",
        "    )(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=5,\n",
        "        activation='relu',\n",
        "        padding='same',\n",
        "    )(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(1, 13),\n",
        "        dilation_rate=(1, 60),\n",
        "        activation='relu',\n",
        "    )(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(\n",
        "        filters=1,\n",
        "        kernel_size=(70, 1),\n",
        "        activation='sigmoid'\n",
        "    )(x)\n",
        "\n",
        "    output = tf.keras.layers.Flatten()(x)\n",
        "    \n",
        "    discriminator = tf.keras.models.Model([history, target], output, name='discriminator')\n",
        "\n",
        "    return discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rS0oSql4LtX"
      },
      "outputs": [],
      "source": [
        "class RadarGAN(tf.keras.Model):\n",
        "    def __init__(self, discriminator, generator):\n",
        "        super(RadarGAN, self).__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "        self.gen_mae_tracker = tf.keras.metrics.Mean(name=\"generator_mae\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.gen_loss_tracker, self.disc_loss_tracker, self.gen_mae_tracker]\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(RadarGAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.mae = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def train_step(self, data):\n",
        "\n",
        "        # класс 1 - настоящие данные, класс 0 - дискриминированные данные\n",
        "        x, y = data\n",
        "        x_shape = tf.shape(x)\n",
        "        y_shape = tf.shape(y)\n",
        "\n",
        "        # обучение дискриминатора\n",
        "        for step in range(1):\n",
        "            \n",
        "            # маска по Бернулли\n",
        "            p = tf.constant([0.8])\n",
        "            r = tf.random.uniform(shape=x_shape, maxval=1)\n",
        "            b = tf.math.greater(p, r)\n",
        "            f = tf.cast(b, dtype=tf.dtypes.float64)\n",
        "\n",
        "            # выход генератора\n",
        "            generated = tf.cast(self.generator(tf.math.multiply(x, f)), dtype=tf.dtypes.float64)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                real = self.discriminator([x, y])\n",
        "                fake = self.discriminator([x, generated])\n",
        "                \n",
        "                # значения потерь на чистых данных\n",
        "                loss_on_real = self.loss_fn(tf.ones_like(real), real)\n",
        "                loss_on_fake = self.loss_fn(tf.zeros_like(fake), fake)\n",
        "                \n",
        "                # целевое значение дискриминатора\n",
        "                real_mixed = tf.math.round(tf.random.uniform(shape=(y_shape[0], y_shape[2]), minval=0, maxval=1, dtype=tf.dtypes.float64))\n",
        "\n",
        "                # маска для реальных данных\n",
        "                real_mask_mixed = tf.reshape(real_mixed, shape=(y_shape[0], 1, y_shape[2], 1))\n",
        "                real_mask_mixed = tf.repeat(real_mask_mixed, repeats=y_shape[1], axis=1)\n",
        "                real_mask_mixed = tf.repeat(real_mask_mixed, repeats=y_shape[3], axis=3)\n",
        "                # маска для фейковых данных\n",
        "                fake_mask_mixed = tf.math.subtract(tf.ones(shape=tf.shape(real_mask_mixed), dtype=tf.dtypes.float64), real_mask_mixed)\n",
        "\n",
        "                # перемешивание данных\n",
        "                mixed = tf.math.add(tf.math.multiply(real_mask_mixed, y), tf.math.multiply(fake_mask_mixed, generated))\n",
        "\n",
        "                predictions = self.discriminator([x, mixed])\n",
        "\n",
        "                loss_on_mixed = self.loss_fn(real, predictions)\n",
        "\n",
        "                d_loss = loss_on_real + loss_on_fake + loss_on_mixed\n",
        "\n",
        "            grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(grads, self.discriminator.trainable_weights)\n",
        "            )\n",
        "        \n",
        "        # таргет генератора как выход дискриминатора\n",
        "        g_target = tf.ones(shape=tf.shape(real), dtype=tf.dtypes.float64)\n",
        "\n",
        "        # тренировка генератора\n",
        "        for step in range(1):\n",
        "            with tf.GradientTape() as tape:\n",
        "\n",
        "                # Бернулли\n",
        "                p = tf.constant([0.8])\n",
        "                r = tf.random.uniform(shape=x_shape, maxval=1)\n",
        "                b = tf.math.greater(p, r)\n",
        "                f = tf.cast(b, dtype=tf.dtypes.float64)\n",
        "\n",
        "                generated = tf.cast(self.generator(tf.math.multiply(x, f)), dtype=tf.dtypes.float64)\n",
        "\n",
        "                discriminated = self.discriminator([x, generated])\n",
        "                g_loss = self.loss_fn(g_target, discriminated)\n",
        "                g_mae = self.mae(y, generated)\n",
        "                \n",
        "            grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "            self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        # Monitor loss.\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        self.gen_mae_tracker.update_state(g_mae)\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "            \"g_mae\": self.gen_mae_tracker.result()\n",
        "            }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        y_shape = tf.shape(y)\n",
        "\n",
        "        # Compute predictions\n",
        "        generated = self.generator(x)\n",
        "        real = self.discriminator([x, y], training=False)\n",
        "        fake = self.discriminator([x, generated], training=False)\n",
        "\n",
        "        # Compute losses\n",
        "        g_target = tf.ones(shape=tf.shape(real), dtype=tf.dtypes.float64)\n",
        "        g_loss = self.loss_fn(g_target, fake)\n",
        "        g_mae = self.mae(y, generated)\n",
        "\n",
        "        loss_on_real = self.loss_fn(tf.ones_like(real), real)\n",
        "        loss_on_fake = self.loss_fn(tf.zeros_like(fake), fake)\n",
        "        \n",
        "        d_loss = loss_on_real + loss_on_fake\n",
        "\n",
        "        # Update losses\n",
        "        self.gen_loss_tracker.update_state(g_loss)\n",
        "        self.disc_loss_tracker.update_state(d_loss)\n",
        "        self.gen_mae_tracker.update_state(g_mae)\n",
        "\n",
        "        return {\n",
        "            \"g_loss\": self.gen_loss_tracker.result(),\n",
        "            \"d_loss\": self.disc_loss_tracker.result(),\n",
        "            \"g_mae\": self.gen_mae_tracker.result()\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5acxn9A7q5g"
      },
      "outputs": [],
      "source": [
        "# если обучать с нуля\n",
        "radar_gan = RadarGAN(discriminator=get_discriminator(), generator=get_generator())\n",
        "\n",
        "# дообучение\n",
        "# radar_gan = RadarGAN(discriminator=keras.models.load_model('drive/MyDrive/radargan_discriminator_new.hdf5'), generator=keras.models.load_model('drive/MyDrive/radargan_generator_new.hdf5'))\n",
        "\n",
        "# графическое изображение структуры\n",
        "tf.keras.utils.plot_model(radar_gan.generator, to_file='g.png', show_shapes=True)\n",
        "tf.keras.utils.plot_model(radar_gan.discriminator, to_file='d.png', show_shapes=True)\n",
        "\n",
        "radar_gan.compile(\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=tf.keras.losses.BinaryCrossentropy()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXTIsCCW4MV6",
        "outputId": "837b4146-b5f1-4458-db7d-b89a4fa6b178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 100, 5040, 1)]    0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 100, 5040, 32)     832       \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 100, 720, 32)      7200      \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 100, 720, 32)      25632     \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 100, 60, 32)       12320     \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 100, 60, 1)        33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46,017\n",
            "Trainable params: 46,017\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "radar_gan.generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKy9lAuM4T1W",
        "outputId": "bf6e3655-4f7d-4bcd-a443-d639474f1f7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 100, 5040,   0           []                               \n",
            "                                1)]                                                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 100, 60, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 100, 5100, 1  0           ['input_3[0][0]',                \n",
            "                                )                                 'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 100, 5100, 3  832         ['concatenate_1[0][0]']          \n",
            "                                2)                                                                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 100, 780, 32  7200        ['conv2d[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 100, 780, 32  25632       ['conv2d_1[0][0]']               \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 100, 60, 32)  13344       ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 31, 60, 1)    2241        ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 1860)         0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 49,249\n",
            "Trainable params: 49,249\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "radar_gan.discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4a-x3uz4iDn",
        "outputId": "ae66bb78-4677-4c3a-afec-454035c14aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1960 - d_loss: 1.4026 - g_mae: 1.8423 - val_g_loss: 1.9202 - val_d_loss: 1.3016 - val_g_mae: 1.8227\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0030 - d_loss: 1.4401 - g_mae: 1.8960 - val_g_loss: 1.3407 - val_d_loss: 1.3106 - val_g_mae: 1.5516\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7140 - d_loss: 1.3418 - g_mae: 1.6759 - val_g_loss: 1.6609 - val_d_loss: 1.4277 - val_g_mae: 1.5711\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.3404 - d_loss: 1.6602 - g_mae: 1.9297 - val_g_loss: 1.1760 - val_d_loss: 2.6701 - val_g_mae: 3.1424\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8931 - d_loss: 2.1845 - g_mae: 3.5330 - val_g_loss: 1.1427 - val_d_loss: 1.4504 - val_g_mae: 3.4625\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6819 - d_loss: 2.1159 - g_mae: 2.4398 - val_g_loss: 0.7488 - val_d_loss: 1.4994 - val_g_mae: 1.5343\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8829 - d_loss: 2.1865 - g_mae: 1.6364 - val_g_loss: 0.8598 - val_d_loss: 1.3705 - val_g_mae: 1.4015\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6727 - d_loss: 2.0219 - g_mae: 1.6343 - val_g_loss: 0.5699 - val_d_loss: 1.3145 - val_g_mae: 1.3878\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6959 - d_loss: 1.8782 - g_mae: 1.5503 - val_g_loss: 0.6994 - val_d_loss: 1.2010 - val_g_mae: 1.4051\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7076 - d_loss: 1.7073 - g_mae: 1.6333 - val_g_loss: 0.8402 - val_d_loss: 1.1071 - val_g_mae: 1.4291\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0984 - d_loss: 1.3516 - g_mae: 1.6347 - val_g_loss: 1.7263 - val_d_loss: 1.1541 - val_g_mae: 1.4084\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.3825 - d_loss: 0.9752 - g_mae: 1.6687 - val_g_loss: 2.0107 - val_d_loss: 1.2598 - val_g_mae: 1.4050\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.8225 - d_loss: 0.9836 - g_mae: 1.6816 - val_g_loss: 1.7461 - val_d_loss: 1.2656 - val_g_mae: 1.4129\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0165 - d_loss: 1.1842 - g_mae: 1.5508 - val_g_loss: 1.4693 - val_d_loss: 1.3118 - val_g_mae: 2.3550\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3738 - d_loss: 1.7360 - g_mae: 3.0566 - val_g_loss: 1.1008 - val_d_loss: 0.9890 - val_g_mae: 7.6275\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9196 - d_loss: 1.5391 - g_mae: 7.8014 - val_g_loss: 1.1882 - val_d_loss: 0.8903 - val_g_mae: 9.9316\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0735 - d_loss: 1.2283 - g_mae: 8.5616 - val_g_loss: 1.5040 - val_d_loss: 0.9056 - val_g_mae: 5.4396\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.2251 - d_loss: 1.4174 - g_mae: 3.7692 - val_g_loss: 1.5057 - val_d_loss: 1.0657 - val_g_mae: 1.9771\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1373 - d_loss: 1.6356 - g_mae: 1.7521 - val_g_loss: 1.0629 - val_d_loss: 1.0760 - val_g_mae: 1.4639\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9888 - d_loss: 1.4798 - g_mae: 1.5285 - val_g_loss: 1.1961 - val_d_loss: 1.0878 - val_g_mae: 1.4303\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5372 - d_loss: 1.2322 - g_mae: 1.5938 - val_g_loss: 2.0309 - val_d_loss: 1.3088 - val_g_mae: 1.4311\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.4134 - d_loss: 1.4058 - g_mae: 1.6533 - val_g_loss: 1.4764 - val_d_loss: 1.9164 - val_g_mae: 2.2744\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.4373 - d_loss: 1.9322 - g_mae: 2.9244 - val_g_loss: 1.3696 - val_d_loss: 1.0787 - val_g_mae: 5.5968\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9752 - d_loss: 1.7774 - g_mae: 5.7581 - val_g_loss: 1.0095 - val_d_loss: 1.1821 - val_g_mae: 5.9408\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9608 - d_loss: 1.7244 - g_mae: 5.0963 - val_g_loss: 1.0242 - val_d_loss: 1.1661 - val_g_mae: 3.3289\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8952 - d_loss: 1.7871 - g_mae: 2.9556 - val_g_loss: 0.8389 - val_d_loss: 1.2483 - val_g_mae: 1.9435\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7570 - d_loss: 1.8668 - g_mae: 1.9157 - val_g_loss: 0.8540 - val_d_loss: 1.2179 - val_g_mae: 1.5782\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8183 - d_loss: 1.8490 - g_mae: 1.6507 - val_g_loss: 0.8996 - val_d_loss: 1.1823 - val_g_mae: 1.5039\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8390 - d_loss: 1.7825 - g_mae: 1.6953 - val_g_loss: 0.8947 - val_d_loss: 1.1653 - val_g_mae: 1.4739\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8282 - d_loss: 1.7419 - g_mae: 1.6958 - val_g_loss: 0.9624 - val_d_loss: 1.1943 - val_g_mae: 1.4891\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8783 - d_loss: 1.8947 - g_mae: 1.6179 - val_g_loss: 1.0852 - val_d_loss: 1.2937 - val_g_mae: 1.5511\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9845 - d_loss: 2.1019 - g_mae: 1.8564 - val_g_loss: 1.0544 - val_d_loss: 1.3244 - val_g_mae: 1.7613\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0795 - d_loss: 2.0472 - g_mae: 2.2077 - val_g_loss: 0.7489 - val_d_loss: 1.3448 - val_g_mae: 1.7686\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8243 - d_loss: 1.9810 - g_mae: 2.0063 - val_g_loss: 0.5163 - val_d_loss: 1.5359 - val_g_mae: 1.5092\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6393 - d_loss: 2.0722 - g_mae: 1.6558 - val_g_loss: 0.6679 - val_d_loss: 1.4486 - val_g_mae: 1.4593\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7414 - d_loss: 2.0657 - g_mae: 1.6213 - val_g_loss: 0.7505 - val_d_loss: 1.3750 - val_g_mae: 1.4074\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8008 - d_loss: 1.9928 - g_mae: 1.6861 - val_g_loss: 0.8160 - val_d_loss: 1.2249 - val_g_mae: 1.4215\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8987 - d_loss: 1.6896 - g_mae: 1.5687 - val_g_loss: 0.9649 - val_d_loss: 1.0993 - val_g_mae: 1.4146\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1465 - d_loss: 1.3615 - g_mae: 1.6903 - val_g_loss: 1.9034 - val_d_loss: 1.8171 - val_g_mae: 1.5151\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1565 - d_loss: 2.8765 - g_mae: 2.0024 - val_g_loss: 1.9422 - val_d_loss: 2.0381 - val_g_mae: 3.1083\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.8082 - d_loss: 2.3659 - g_mae: 4.1140 - val_g_loss: 0.9006 - val_d_loss: 1.2498 - val_g_mae: 2.5875\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1793 - d_loss: 1.8851 - g_mae: 2.7592 - val_g_loss: 0.4633 - val_d_loss: 1.4745 - val_g_mae: 1.6187\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.5452 - d_loss: 2.0580 - g_mae: 1.7048 - val_g_loss: 0.5152 - val_d_loss: 1.4653 - val_g_mae: 1.4964\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6044 - d_loss: 2.0920 - g_mae: 1.6633 - val_g_loss: 0.6252 - val_d_loss: 1.4266 - val_g_mae: 1.4490\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6835 - d_loss: 2.0912 - g_mae: 1.5773 - val_g_loss: 0.6899 - val_d_loss: 1.4141 - val_g_mae: 1.4586\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7278 - d_loss: 2.0931 - g_mae: 1.5364 - val_g_loss: 0.7128 - val_d_loss: 1.4064 - val_g_mae: 1.4393\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7364 - d_loss: 2.0879 - g_mae: 1.5682 - val_g_loss: 0.7188 - val_d_loss: 1.4012 - val_g_mae: 1.4380\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7358 - d_loss: 2.0866 - g_mae: 1.5111 - val_g_loss: 0.7146 - val_d_loss: 1.3970 - val_g_mae: 1.4368\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7253 - d_loss: 2.0824 - g_mae: 1.5672 - val_g_loss: 0.7081 - val_d_loss: 1.3938 - val_g_mae: 1.4239\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7197 - d_loss: 2.0812 - g_mae: 1.5453 - val_g_loss: 0.7018 - val_d_loss: 1.3908 - val_g_mae: 1.4377\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7131 - d_loss: 2.0783 - g_mae: 1.6652 - val_g_loss: 0.6962 - val_d_loss: 1.3885 - val_g_mae: 1.4357\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7061 - d_loss: 2.0756 - g_mae: 1.6568 - val_g_loss: 0.6961 - val_d_loss: 1.3858 - val_g_mae: 1.4096\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7081 - d_loss: 2.0729 - g_mae: 1.5434 - val_g_loss: 0.6997 - val_d_loss: 1.3822 - val_g_mae: 1.4201\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7208 - d_loss: 2.0670 - g_mae: 1.6785 - val_g_loss: 0.7103 - val_d_loss: 1.3768 - val_g_mae: 1.4056\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7293 - d_loss: 2.0608 - g_mae: 1.6147 - val_g_loss: 0.7004 - val_d_loss: 1.3699 - val_g_mae: 1.4044\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7142 - d_loss: 2.0492 - g_mae: 1.5798 - val_g_loss: 0.6582 - val_d_loss: 1.3619 - val_g_mae: 1.4542\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6870 - d_loss: 2.0310 - g_mae: 1.5521 - val_g_loss: 0.6546 - val_d_loss: 1.3447 - val_g_mae: 1.4167\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6860 - d_loss: 1.9951 - g_mae: 1.5923 - val_g_loss: 0.6558 - val_d_loss: 1.3116 - val_g_mae: 1.4312\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.6999 - d_loss: 1.9349 - g_mae: 1.6238 - val_g_loss: 0.7230 - val_d_loss: 1.2502 - val_g_mae: 1.4600\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7362 - d_loss: 1.8304 - g_mae: 1.5681 - val_g_loss: 0.7288 - val_d_loss: 1.2083 - val_g_mae: 1.4507\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8465 - d_loss: 1.6943 - g_mae: 1.5635 - val_g_loss: 1.1221 - val_d_loss: 1.1842 - val_g_mae: 1.4537\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0436 - d_loss: 1.6224 - g_mae: 1.6168 - val_g_loss: 1.3478 - val_d_loss: 1.3833 - val_g_mae: 1.5391\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3596 - d_loss: 1.9912 - g_mae: 1.8211 - val_g_loss: 1.7650 - val_d_loss: 2.0595 - val_g_mae: 1.9818\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1794 - d_loss: 2.9878 - g_mae: 2.7675 - val_g_loss: 1.2636 - val_d_loss: 1.9512 - val_g_mae: 3.9016\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9551 - d_loss: 1.8983 - g_mae: 5.1481 - val_g_loss: 0.7591 - val_d_loss: 1.3596 - val_g_mae: 3.7692\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3523 - d_loss: 1.4835 - g_mae: 3.9614 - val_g_loss: 0.5272 - val_d_loss: 1.5858 - val_g_mae: 2.1795\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0453 - d_loss: 1.7531 - g_mae: 2.3753 - val_g_loss: 0.7919 - val_d_loss: 1.4640 - val_g_mae: 1.6179\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8257 - d_loss: 2.0031 - g_mae: 1.7444 - val_g_loss: 0.7858 - val_d_loss: 1.4153 - val_g_mae: 1.4675\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7906 - d_loss: 1.9662 - g_mae: 1.6504 - val_g_loss: 0.7930 - val_d_loss: 1.3356 - val_g_mae: 1.4440\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8148 - d_loss: 1.8124 - g_mae: 1.6151 - val_g_loss: 0.8543 - val_d_loss: 1.3504 - val_g_mae: 1.4395\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9942 - d_loss: 1.5862 - g_mae: 1.5943 - val_g_loss: 1.1828 - val_d_loss: 1.3469 - val_g_mae: 1.4725\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3409 - d_loss: 1.4318 - g_mae: 1.5160 - val_g_loss: 1.4143 - val_d_loss: 1.3275 - val_g_mae: 1.4331\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5551 - d_loss: 1.2716 - g_mae: 1.6078 - val_g_loss: 1.5371 - val_d_loss: 1.2752 - val_g_mae: 1.4198\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6481 - d_loss: 1.2307 - g_mae: 1.5858 - val_g_loss: 1.7975 - val_d_loss: 1.2976 - val_g_mae: 1.4834\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0296 - d_loss: 1.1600 - g_mae: 1.7629 - val_g_loss: 1.8651 - val_d_loss: 1.3747 - val_g_mae: 1.5978\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0204 - d_loss: 1.2615 - g_mae: 1.8906 - val_g_loss: 1.7106 - val_d_loss: 1.4966 - val_g_mae: 1.8581\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6963 - d_loss: 1.4862 - g_mae: 2.0876 - val_g_loss: 1.6166 - val_d_loss: 1.6961 - val_g_mae: 2.3750\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8649 - d_loss: 1.5767 - g_mae: 2.9680 - val_g_loss: 0.8320 - val_d_loss: 1.7005 - val_g_mae: 3.1503\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7467 - d_loss: 1.4333 - g_mae: 3.7795 - val_g_loss: 0.7006 - val_d_loss: 1.4114 - val_g_mae: 3.4821\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3690 - d_loss: 1.3077 - g_mae: 3.6396 - val_g_loss: 0.8805 - val_d_loss: 1.0736 - val_g_mae: 2.6979\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1656 - d_loss: 1.3967 - g_mae: 2.6182 - val_g_loss: 0.9495 - val_d_loss: 1.0690 - val_g_mae: 1.8806\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0618 - d_loss: 1.4315 - g_mae: 1.8909 - val_g_loss: 0.9495 - val_d_loss: 1.1954 - val_g_mae: 1.5297\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3154 - d_loss: 1.3026 - g_mae: 1.6572 - val_g_loss: 1.4624 - val_d_loss: 1.2608 - val_g_mae: 1.4209\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7060 - d_loss: 1.1724 - g_mae: 1.7029 - val_g_loss: 1.7486 - val_d_loss: 1.2542 - val_g_mae: 1.3761\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0331 - d_loss: 1.0494 - g_mae: 1.7144 - val_g_loss: 1.6664 - val_d_loss: 1.2394 - val_g_mae: 1.3934\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8372 - d_loss: 1.1199 - g_mae: 1.4407 - val_g_loss: 2.1684 - val_d_loss: 1.2442 - val_g_mae: 1.4029\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1280 - d_loss: 1.0187 - g_mae: 1.6317 - val_g_loss: 2.1397 - val_d_loss: 1.2495 - val_g_mae: 1.4292\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.2927 - d_loss: 0.9845 - g_mae: 1.6394 - val_g_loss: 1.9705 - val_d_loss: 1.3014 - val_g_mae: 1.4304\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.2431 - d_loss: 1.0828 - g_mae: 1.5937 - val_g_loss: 1.9772 - val_d_loss: 1.3494 - val_g_mae: 1.5938\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1757 - d_loss: 1.1934 - g_mae: 1.6875 - val_g_loss: 1.6352 - val_d_loss: 1.9074 - val_g_mae: 1.9877\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.2176 - d_loss: 1.6223 - g_mae: 2.3429 - val_g_loss: 2.1280 - val_d_loss: 0.9781 - val_g_mae: 4.2173\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0071 - d_loss: 1.3096 - g_mae: 4.1458 - val_g_loss: 1.7513 - val_d_loss: 0.9150 - val_g_mae: 4.1951\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5664 - d_loss: 1.3114 - g_mae: 3.3904 - val_g_loss: 1.1547 - val_d_loss: 1.1340 - val_g_mae: 2.3739\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0871 - d_loss: 1.5475 - g_mae: 1.9862 - val_g_loss: 1.2726 - val_d_loss: 1.1156 - val_g_mae: 1.5790\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0464 - d_loss: 1.4686 - g_mae: 1.5678 - val_g_loss: 1.2947 - val_d_loss: 1.1442 - val_g_mae: 1.4589\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5731 - d_loss: 1.2877 - g_mae: 1.6341 - val_g_loss: 1.5680 - val_d_loss: 1.2066 - val_g_mae: 1.4300\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9758 - d_loss: 1.0952 - g_mae: 1.6094 - val_g_loss: 1.7968 - val_d_loss: 1.1992 - val_g_mae: 1.4202\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1238 - d_loss: 1.0317 - g_mae: 1.5844 - val_g_loss: 1.8362 - val_d_loss: 1.2979 - val_g_mae: 1.4404\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.4232 - d_loss: 1.0476 - g_mae: 1.7106 - val_g_loss: 1.4442 - val_d_loss: 1.7654 - val_g_mae: 1.6150\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7786 - d_loss: 1.4666 - g_mae: 1.6586 - val_g_loss: 1.6816 - val_d_loss: 1.2651 - val_g_mae: 2.7267\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6790 - d_loss: 1.5599 - g_mae: 2.7417 - val_g_loss: 1.3371 - val_d_loss: 0.9485 - val_g_mae: 4.3176\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.2810 - d_loss: 1.3905 - g_mae: 4.0459 - val_g_loss: 1.0730 - val_d_loss: 1.0003 - val_g_mae: 4.3198\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1455 - d_loss: 1.3486 - g_mae: 3.6571 - val_g_loss: 1.0725 - val_d_loss: 1.0612 - val_g_mae: 3.1179\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0501 - d_loss: 1.4705 - g_mae: 2.4681 - val_g_loss: 0.9737 - val_d_loss: 1.1278 - val_g_mae: 1.8748\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9121 - d_loss: 1.7126 - g_mae: 1.8474 - val_g_loss: 1.0264 - val_d_loss: 1.1169 - val_g_mae: 1.4798\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0821 - d_loss: 1.5601 - g_mae: 1.5340 - val_g_loss: 1.2729 - val_d_loss: 1.0806 - val_g_mae: 1.4532\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3599 - d_loss: 1.1996 - g_mae: 1.5381 - val_g_loss: 1.5877 - val_d_loss: 1.2138 - val_g_mae: 1.4312\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0438 - d_loss: 1.1511 - g_mae: 1.6367 - val_g_loss: 1.6839 - val_d_loss: 1.5972 - val_g_mae: 1.5416\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7183 - d_loss: 1.6814 - g_mae: 1.7438 - val_g_loss: 1.4473 - val_d_loss: 2.0834 - val_g_mae: 2.2798\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8782 - d_loss: 2.2075 - g_mae: 2.6661 - val_g_loss: 2.2653 - val_d_loss: 1.0740 - val_g_mae: 3.8753\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.2945 - d_loss: 1.6685 - g_mae: 3.9627 - val_g_loss: 0.8134 - val_d_loss: 1.1350 - val_g_mae: 3.1214\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.7749 - d_loss: 1.6236 - g_mae: 2.7345 - val_g_loss: 0.6953 - val_d_loss: 1.2113 - val_g_mae: 2.3480\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9112 - d_loss: 1.7279 - g_mae: 2.0397 - val_g_loss: 0.9021 - val_d_loss: 1.1485 - val_g_mae: 1.9591\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8569 - d_loss: 1.6995 - g_mae: 1.9145 - val_g_loss: 0.7142 - val_d_loss: 1.2099 - val_g_mae: 1.7943\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.8709 - d_loss: 1.5818 - g_mae: 1.6916 - val_g_loss: 1.0406 - val_d_loss: 1.1688 - val_g_mae: 1.7065\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1818 - d_loss: 1.4472 - g_mae: 1.7342 - val_g_loss: 1.0786 - val_d_loss: 1.3725 - val_g_mae: 1.7838\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3210 - d_loss: 1.4714 - g_mae: 1.7107 - val_g_loss: 1.2381 - val_d_loss: 1.4717 - val_g_mae: 2.0396\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3310 - d_loss: 1.5922 - g_mae: 1.9404 - val_g_loss: 1.2261 - val_d_loss: 1.2801 - val_g_mae: 2.6095\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.2018 - d_loss: 1.6008 - g_mae: 2.3940 - val_g_loss: 1.2657 - val_d_loss: 1.1474 - val_g_mae: 2.9281\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1539 - d_loss: 1.7023 - g_mae: 2.4347 - val_g_loss: 1.2155 - val_d_loss: 1.1776 - val_g_mae: 2.4896\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0248 - d_loss: 1.7703 - g_mae: 2.2198 - val_g_loss: 1.1192 - val_d_loss: 1.2121 - val_g_mae: 1.9407\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0911 - d_loss: 1.7658 - g_mae: 1.8754 - val_g_loss: 0.9714 - val_d_loss: 1.2996 - val_g_mae: 1.7494\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0643 - d_loss: 1.7010 - g_mae: 1.7684 - val_g_loss: 1.0728 - val_d_loss: 1.3361 - val_g_mae: 1.7012\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1362 - d_loss: 1.7066 - g_mae: 1.7796 - val_g_loss: 1.0081 - val_d_loss: 1.5165 - val_g_mae: 1.8650\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.0786 - d_loss: 1.8214 - g_mae: 1.8791 - val_g_loss: 0.9468 - val_d_loss: 1.4985 - val_g_mae: 2.2387\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1399 - d_loss: 1.6956 - g_mae: 2.3581 - val_g_loss: 0.9173 - val_d_loss: 1.5185 - val_g_mae: 2.4328\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1085 - d_loss: 1.6553 - g_mae: 2.2721 - val_g_loss: 0.9915 - val_d_loss: 1.2678 - val_g_mae: 1.9965\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1377 - d_loss: 1.6255 - g_mae: 1.8771 - val_g_loss: 0.8894 - val_d_loss: 1.3667 - val_g_mae: 1.6992\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.2112 - d_loss: 1.5517 - g_mae: 1.6562 - val_g_loss: 0.8262 - val_d_loss: 1.4695 - val_g_mae: 1.7166\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1567 - d_loss: 1.4681 - g_mae: 1.8345 - val_g_loss: 0.8858 - val_d_loss: 1.8943 - val_g_mae: 1.9315\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1504 - d_loss: 1.5834 - g_mae: 1.9923 - val_g_loss: 0.8693 - val_d_loss: 1.7124 - val_g_mae: 2.4632\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.1500 - d_loss: 1.5566 - g_mae: 2.2281 - val_g_loss: 0.9801 - val_d_loss: 1.4026 - val_g_mae: 2.7182\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.2030 - d_loss: 1.4850 - g_mae: 2.2558 - val_g_loss: 0.8334 - val_d_loss: 1.5158 - val_g_mae: 2.4828\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.2467 - d_loss: 1.3821 - g_mae: 2.2833 - val_g_loss: 0.8338 - val_d_loss: 1.3999 - val_g_mae: 2.5046\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3164 - d_loss: 1.3006 - g_mae: 2.1797 - val_g_loss: 1.0489 - val_d_loss: 1.2210 - val_g_mae: 2.2434\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.4179 - d_loss: 1.2777 - g_mae: 1.8826 - val_g_loss: 1.0036 - val_d_loss: 1.3875 - val_g_mae: 1.9864\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7530 - d_loss: 1.1410 - g_mae: 1.9277 - val_g_loss: 0.9750 - val_d_loss: 1.5368 - val_g_mae: 1.9077\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7029 - d_loss: 1.1649 - g_mae: 1.8044 - val_g_loss: 0.9895 - val_d_loss: 1.5129 - val_g_mae: 1.9546\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6750 - d_loss: 1.1626 - g_mae: 1.8444 - val_g_loss: 1.0854 - val_d_loss: 1.6238 - val_g_mae: 2.1483\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8134 - d_loss: 1.1747 - g_mae: 1.9639 - val_g_loss: 0.8738 - val_d_loss: 1.8667 - val_g_mae: 2.2813\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8856 - d_loss: 1.2065 - g_mae: 1.9382 - val_g_loss: 1.1230 - val_d_loss: 1.6774 - val_g_mae: 2.2158\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0266 - d_loss: 1.1580 - g_mae: 2.0047 - val_g_loss: 1.1140 - val_d_loss: 1.6181 - val_g_mae: 2.1438\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9692 - d_loss: 1.1391 - g_mae: 1.9845 - val_g_loss: 1.0542 - val_d_loss: 1.7009 - val_g_mae: 1.9871\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8190 - d_loss: 1.2267 - g_mae: 1.6887 - val_g_loss: 1.3081 - val_d_loss: 1.4857 - val_g_mae: 1.7966\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0478 - d_loss: 1.1706 - g_mae: 1.7306 - val_g_loss: 1.3566 - val_d_loss: 1.4574 - val_g_mae: 1.6040\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1055 - d_loss: 1.2200 - g_mae: 1.6761 - val_g_loss: 1.6229 - val_d_loss: 1.5399 - val_g_mae: 1.5428\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1397 - d_loss: 1.2123 - g_mae: 1.6331 - val_g_loss: 1.6618 - val_d_loss: 1.5236 - val_g_mae: 1.4506\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1279 - d_loss: 1.1953 - g_mae: 1.7867 - val_g_loss: 1.5185 - val_d_loss: 1.8834 - val_g_mae: 1.6780\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.2115 - d_loss: 1.6445 - g_mae: 2.1831 - val_g_loss: 1.8120 - val_d_loss: 1.8509 - val_g_mae: 3.4075\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.9958 - d_loss: 1.5711 - g_mae: 3.6611 - val_g_loss: 1.6275 - val_d_loss: 1.3743 - val_g_mae: 2.9688\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0382 - d_loss: 1.4062 - g_mae: 2.4113 - val_g_loss: 1.3817 - val_d_loss: 1.3386 - val_g_mae: 1.9845\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5473 - d_loss: 1.5112 - g_mae: 1.8744 - val_g_loss: 1.4376 - val_d_loss: 1.2703 - val_g_mae: 1.7129\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6165 - d_loss: 1.3419 - g_mae: 1.8040 - val_g_loss: 1.2536 - val_d_loss: 1.3284 - val_g_mae: 1.6379\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6422 - d_loss: 1.2248 - g_mae: 1.7014 - val_g_loss: 1.2511 - val_d_loss: 1.2812 - val_g_mae: 1.6981\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5085 - d_loss: 1.2485 - g_mae: 1.6009 - val_g_loss: 1.4709 - val_d_loss: 1.2214 - val_g_mae: 1.8905\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6433 - d_loss: 1.1854 - g_mae: 1.9494 - val_g_loss: 1.6835 - val_d_loss: 1.1332 - val_g_mae: 2.1204\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8744 - d_loss: 1.1410 - g_mae: 2.0393 - val_g_loss: 1.5812 - val_d_loss: 1.1664 - val_g_mae: 1.8916\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9072 - d_loss: 1.1474 - g_mae: 1.8929 - val_g_loss: 1.6099 - val_d_loss: 1.2271 - val_g_mae: 1.6409\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9562 - d_loss: 1.1935 - g_mae: 1.7058 - val_g_loss: 1.4944 - val_d_loss: 1.3478 - val_g_mae: 1.5826\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8562 - d_loss: 1.3172 - g_mae: 1.5835 - val_g_loss: 1.5149 - val_d_loss: 1.4913 - val_g_mae: 1.6646\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8466 - d_loss: 1.3712 - g_mae: 1.8911 - val_g_loss: 1.1905 - val_d_loss: 1.5169 - val_g_mae: 1.9998\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6438 - d_loss: 1.4219 - g_mae: 2.1796 - val_g_loss: 1.3505 - val_d_loss: 1.3170 - val_g_mae: 2.2338\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9679 - d_loss: 1.3753 - g_mae: 2.1722 - val_g_loss: 1.5259 - val_d_loss: 1.3881 - val_g_mae: 1.7303\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1489 - d_loss: 1.3519 - g_mae: 1.8431 - val_g_loss: 1.9197 - val_d_loss: 1.4838 - val_g_mae: 1.5040\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.1585 - d_loss: 1.2869 - g_mae: 1.6845 - val_g_loss: 1.6666 - val_d_loss: 1.4226 - val_g_mae: 1.4715\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0953 - d_loss: 1.1738 - g_mae: 1.7058 - val_g_loss: 1.6916 - val_d_loss: 1.4269 - val_g_mae: 1.4768\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0203 - d_loss: 1.1579 - g_mae: 1.6298 - val_g_loss: 1.7232 - val_d_loss: 1.4393 - val_g_mae: 1.4339\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.2992 - d_loss: 1.1031 - g_mae: 1.7194 - val_g_loss: 1.9259 - val_d_loss: 1.5799 - val_g_mae: 1.4803\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.4572 - d_loss: 1.1882 - g_mae: 1.7695 - val_g_loss: 1.6593 - val_d_loss: 1.9119 - val_g_mae: 1.6672\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.2180 - d_loss: 1.6459 - g_mae: 2.0948 - val_g_loss: 1.4183 - val_d_loss: 2.0770 - val_g_mae: 2.7600\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.8525 - d_loss: 1.6816 - g_mae: 3.2929 - val_g_loss: 1.2637 - val_d_loss: 1.6264 - val_g_mae: 4.0317\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.2817 - d_loss: 1.5482 - g_mae: 3.7130 - val_g_loss: 1.3370 - val_d_loss: 1.1188 - val_g_mae: 3.4429\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3787 - d_loss: 1.5877 - g_mae: 2.9767 - val_g_loss: 0.9773 - val_d_loss: 1.2581 - val_g_mae: 2.2561\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 0.9869 - d_loss: 1.7248 - g_mae: 1.9248 - val_g_loss: 1.1623 - val_d_loss: 1.1961 - val_g_mae: 1.6919\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.3242 - d_loss: 1.5072 - g_mae: 1.6417 - val_g_loss: 1.0913 - val_d_loss: 1.4103 - val_g_mae: 1.5332\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5729 - d_loss: 1.3302 - g_mae: 1.5890 - val_g_loss: 1.2426 - val_d_loss: 1.3724 - val_g_mae: 1.5575\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6093 - d_loss: 1.2850 - g_mae: 1.6721 - val_g_loss: 1.2440 - val_d_loss: 1.5805 - val_g_mae: 1.7805\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.4785 - d_loss: 1.3935 - g_mae: 1.8991 - val_g_loss: 1.1303 - val_d_loss: 1.4158 - val_g_mae: 2.5055\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.4745 - d_loss: 1.3410 - g_mae: 2.4553 - val_g_loss: 1.6381 - val_d_loss: 1.1222 - val_g_mae: 2.6460\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5088 - d_loss: 1.3110 - g_mae: 2.2040 - val_g_loss: 1.5973 - val_d_loss: 1.0377 - val_g_mae: 1.9324\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.5634 - d_loss: 1.2735 - g_mae: 1.7785 - val_g_loss: 1.4229 - val_d_loss: 1.2063 - val_g_mae: 1.6297\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7175 - d_loss: 1.2620 - g_mae: 1.6467 - val_g_loss: 1.4180 - val_d_loss: 1.2697 - val_g_mae: 1.5797\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6560 - d_loss: 1.2470 - g_mae: 1.6674 - val_g_loss: 1.3691 - val_d_loss: 1.3194 - val_g_mae: 1.6595\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7371 - d_loss: 1.2602 - g_mae: 1.8121 - val_g_loss: 1.2904 - val_d_loss: 1.3210 - val_g_mae: 1.8323\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6234 - d_loss: 1.3020 - g_mae: 1.9739 - val_g_loss: 1.5452 - val_d_loss: 1.1847 - val_g_mae: 2.0016\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7456 - d_loss: 1.2620 - g_mae: 2.0914 - val_g_loss: 1.3831 - val_d_loss: 1.2969 - val_g_mae: 1.9252\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7142 - d_loss: 1.2862 - g_mae: 1.9846 - val_g_loss: 1.1793 - val_d_loss: 1.3475 - val_g_mae: 1.7782\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9026 - d_loss: 1.2069 - g_mae: 2.0024 - val_g_loss: 1.4109 - val_d_loss: 1.3563 - val_g_mae: 1.7008\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8202 - d_loss: 1.2075 - g_mae: 1.8210 - val_g_loss: 1.4162 - val_d_loss: 1.3683 - val_g_mae: 1.6406\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8473 - d_loss: 1.2431 - g_mae: 1.6679 - val_g_loss: 1.4550 - val_d_loss: 1.4103 - val_g_mae: 1.6725\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.6957 - d_loss: 1.2545 - g_mae: 1.6765 - val_g_loss: 1.5446 - val_d_loss: 1.4478 - val_g_mae: 1.7143\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9682 - d_loss: 1.1798 - g_mae: 1.8692 - val_g_loss: 1.5202 - val_d_loss: 1.4608 - val_g_mae: 1.8125\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8642 - d_loss: 1.2309 - g_mae: 1.8432 - val_g_loss: 1.6581 - val_d_loss: 1.5585 - val_g_mae: 1.8467\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.9283 - d_loss: 1.2047 - g_mae: 1.8014 - val_g_loss: 1.6182 - val_d_loss: 1.4330 - val_g_mae: 1.8115\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8549 - d_loss: 1.2199 - g_mae: 1.7514 - val_g_loss: 1.7453 - val_d_loss: 1.5189 - val_g_mae: 1.7154\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7602 - d_loss: 1.2357 - g_mae: 1.7939 - val_g_loss: 1.5026 - val_d_loss: 1.3702 - val_g_mae: 1.7668\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7758 - d_loss: 1.2326 - g_mae: 1.8044 - val_g_loss: 1.7057 - val_d_loss: 1.4159 - val_g_mae: 1.7920\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 2.0174 - d_loss: 1.1816 - g_mae: 1.9290 - val_g_loss: 1.2486 - val_d_loss: 1.4205 - val_g_mae: 2.0061\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.7668 - d_loss: 1.2697 - g_mae: 1.9611 - val_g_loss: 1.3328 - val_d_loss: 1.5191 - val_g_mae: 2.0836\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 20s 3s/step - g_loss: 1.8910 - d_loss: 1.2149 - g_mae: 2.1069 - val_g_loss: 1.5436 - val_d_loss: 1.2300 - val_g_mae: 1.9405\n"
          ]
        }
      ],
      "source": [
        "hist = radar_gan.fit(train_dataset, epochs=200, validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kB3aHTc-4swB",
        "outputId": "ed31d935-b3f7-4da8-ce1f-0579aa5e56cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "radar_gan.generator.save('drive/MyDrive/radargan_generator_new.hdf5')\n",
        "radar_gan.discriminator.save('drive/MyDrive/radargan_discriminator_new.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c52mo0QX4YYl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_output(gan, dataset, channel=0, n_examples=5):\n",
        "    dataset = dataset.as_numpy_iterator()\n",
        "    figure, axis = plt.subplots(4, n_examples)\n",
        "\n",
        "    x, y = dataset.next()\n",
        "    for i in range(n_examples):\n",
        "\n",
        "        # шум\n",
        "        # p = tf.constant([0.8])\n",
        "        # r = tf.random.uniform(shape=tf.shape(x), maxval=1)\n",
        "        # b = tf.math.greater(p, r)\n",
        "        # f = tf.cast(b, dtype=tf.dtypes.float64)\n",
        "        # x = tf.math.multiply(x, f)\n",
        "\n",
        "        fake = radar_gan.generator.predict(x)\n",
        "        d_real = radar_gan.discriminator.predict([x, y])\n",
        "        d_fake = radar_gan.discriminator.predict([x, fake])\n",
        "\n",
        "        axis[0, i].imshow(y[i, :, :, channel])\n",
        "        axis[0, i].set_title(\"R\")\n",
        "\n",
        "        axis[1, i].imshow(fake[i, :, :, channel])\n",
        "        axis[1, i].set_title(\"G\")\n",
        "\n",
        "        axis[2, i].imshow((y-fake)[i, :, :, channel])\n",
        "        axis[2, i].set_title(\"R - G\")\n",
        "\n",
        "        axis[3, i].hist((y-fake)[i, :, :, channel])\n",
        "        axis[2, i].set_title(\"R - G distr\")\n",
        "\n",
        "    plt.show()\n",
        "    plt.hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RT736E364umg",
        "outputId": "ac2573b0-11c6-480c-a7a9-fa3f35f4df02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 20 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_output(radar_gan, train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEPpVO2UMoLo"
      },
      "outputs": [],
      "source": [
        "plot_output(radar_gan, val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9ZX1YzC4wKF"
      },
      "outputs": [],
      "source": [
        "for x, y in train_dataset:\n",
        "    fake = radar_gan.generator.predict(x)\n",
        "    print('real: ', radar_gan.discriminator.predict([x, y]))\n",
        "    print('fake: ', radar_gan.discriminator.predict([x, fake]))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp77c5984zhd"
      },
      "outputs": [],
      "source": [
        "for x, y in train_dataset:\n",
        "    plt.figure(figsize=(100, 100))\n",
        "    plt.imshow(x[0,:,:,0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U1MnbY042x3"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history['g_mae'])\n",
        "plt.plot(hist.history['val_g_mae'])\n",
        "plt.title('generator MAE')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGbB2pPc44Pw"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history['g_loss'])\n",
        "plt.plot(hist.history['d_loss'])\n",
        "plt.title('g and d training')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['generator', 'discriminator'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-9l6jkS3_kJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history['val_g_loss'])\n",
        "plt.plot(hist.history['val_d_loss'])\n",
        "plt.title('g and d validation')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['generator', 'discriminator'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDIm4yM3XgnXQt8g0tJJ5w",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}